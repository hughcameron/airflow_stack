kind: pipeline
name: default

steps:
  - name: compose up
    image: docker/compose:1.24.0-rc1
    environment:
      DOCKER_SOCKET:
        from_secret: DOCKER_SOCKET
      DOMAIN:
        from_secret: DOMAIN
      IP_RANGE:
        from_secret: IP_RANGE
      TIMEZONE:
        from_secret: TIMEZONE
      AIRFLOW_EMAIL:
        from_secret: AIRFLOW_EMAIL
      AIRFLOW_USER:
        from_secret: AIRFLOW_USER
      AIRFLOW_FIRSTNAME:
        from_secret: AIRFLOW_FIRSTNAME
      AIRFLOW_LASTNAME:
        from_secret: AIRFLOW_LASTNAME
      AIRFLOW_PASSWORD:
        from_secret: AIRFLOW_PASSWORD
      AIRFLOW_FERNET_KEY:
        from_secret: AIRFLOW_FERNET_KEY
      AIRFLOW_DATABASE_NAME:
        from_secret: AIRFLOW_DATABASE_NAME
      AIRFLOW_DATABASE_USERNAME:
        from_secret: AIRFLOW_DATABASE_USERNAME
      AIRFLOW_DATABASE_PASSWORD:
        from_secret: AIRFLOW_DATABASE_PASSWORD
      WAREHOUSE_NAME:
        from_secret: WAREHOUSE_NAME
      WAREHOUSE_USERNAME:
        from_secret: WAREHOUSE_USERNAME
      WAREHOUSE_PASSWORD:
        from_secret: WAREHOUSE_PASSWORD
      MINIO_ACCESS_KEY:
        from_secret: MINIO_ACCESS_KEY
      MINIO_SECRET_KEY:
        from_secret: MINIO_SECRET_KEY
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    commands:
      - for i in ` docker network inspect -f '{{range .Containers}}{{.Name}} {{end}}' airflowstack_airflow`; do docker network disconnect -f airflowstack_airflow $i; done;
      - docker-compose -p airflowstack down
      - docker pull bitnami/postgresql:latest
      - docker pull bitnami/postgresql:latest
      - docker pull bitnami/redis:latest
      - docker pull bitnami/airflow:latest
      - docker pull bitnami/airflow-scheduler:latest
      - docker pull bitnami/airflow-worker:latest
      - docker pull metabase/metabase
      - docker pull minio/minio
      - docker volume rm airflowstack_airflow_data
      - docker volume rm airflowstack_airflow_database_data
      - docker volume rm airflowstack_airflow_scheduler_data
      - docker volume rm airflowstack_airflow_worker_data
      - docker volume rm airflowstack_minio_data
      - docker volume rm airflowstack_redis_data
      - docker volume rm airflowstack_warehouse_data
      - docker-compose -p airflowstack up -d

  - name: connections
    image: docker:dind
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    environment:
      IP_RANGE:
        from_secret: IP_RANGE
      WAREHOUSE_HOST: 
        from_secret: WAREHOUSE_HOST
      WAREHOUSE_USERNAME: 
        from_secret: WAREHOUSE_USERNAME
      WAREHOUSE_PASSWORD: 
        from_secret: WAREHOUSE_PASSWORD
      WAREHOUSE_NAME: 
        from_secret: WAREHOUSE_NAME
      MINIO_ACCESS_KEY: 
        from_secret: MINIO_ACCESS_KEY
      MINIO_SECRET_KEY: 
        from_secret: MINIO_SECRET_KEY
    commands: 
        - sleep 5m
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id aws_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id azure_cosmos_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id azure_data_lake_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id beeline_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id bigquery_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id cassandra_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id databricks_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id druid_broker_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id druid_ingest_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id emr_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id fs_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id google_cloud_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id hive_cli_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id hiveserver2_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id http_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id local_mysql
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id metastore_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id mongo_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id mssql_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id mysql_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id postgres_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id presto_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id qubole_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id redis_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id segment_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id sftp_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id spark_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id sqlite_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id sqoop_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id ssh_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id vertica_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id wasb_default
        - docker exec -d --user 1001 airflow airflow connections -d --conn_id webhdfs_default
        - docker exec -d --user 1001 airflow airflow connections -a --conn_id warehouse --conn_type postgres --conn_login $WAREHOUSE_USERNAME --conn_password $WAREHOUSE_PASSWORD --conn_host $IP_RANGE.120 --conn_schema $WAREHOUSE_NAME --conn_port 5432
        - docker exec -d --user 1001 airflow airflow connections -a --conn_id minio --conn_type s3 --conn_login $MINIO_ACCESS_KEY --conn_password $MINIO_SECRET_KEY --conn_extra '{"host":"http://'$IP_RANGE'.130:9000"}'

  # - name: requirements
  #   image: docker:dind
  #   volumes:
  #     - name: dockersock
  #       path: /var/run/docker.sock
  #   commands:
  #     - docker cp requirements.txt airflow:/tmp/
  #     - docker exec --user root airflow /opt/bitnami/airflow/venv/bin/pip install -r /tmp/requirements.txt
  #     - docker exec --user root airflow rm /tmp/requirements.txt

  #     - docker cp requirements.txt airflow-worker:/tmp/
  #     - docker exec --user root airflow-worker /opt/bitnami/airflow/venv/bin/pip install -r /tmp/requirements.txt
  #     - docker exec --user root airflow-worker rm /tmp/requirements.txt

  #     - docker cp requirements.txt airflow-scheduler:/tmp/
  #     - docker exec --user root airflow-scheduler /opt/bitnami/airflow/venv/bin/pip install -r /tmp/requirements.txt
  #     - docker exec --user root airflow-scheduler rm /tmp/requirements.txt

  - name: jupyterlab
    image: docker:dind
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
    environment:
      DAGGER_NOTEBOOK_PASSWORD:
        from_secret: DAGGER_NOTEBOOK_PASSWORD
    commands:
      - docker exec --user root airflow-worker mkdir /home/dagger
      - docker exec --user root airflow-worker /opt/bitnami/airflow/venv/bin/pip install jupyterlab
      - docker exec -d --user root airflow-worker jupyter lab --ip=0.0.0.0 --port=2000 --allow-root -y --notebook-dir='/home/dagger' --NotebookApp.password=$DAGGER_NOTEBOOK_PASSWORD

volumes:
  - name: dockersock
    host:
      path: /volume1/docker/config/docker.sock
